{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Tweet by brand\n",
    "\n",
    "### \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tweepy - Python library for accessing the Twitter API.\n",
    "import tweepy\n",
    "\n",
    "# TextBlob - Python library for processing textual data\n",
    "from textblob import TextBlob\n",
    "\n",
    "# WordCloud - Python linrary for creating image wordclouds\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# NLTK - NLP library for sentiment analysis\n",
    "import nltk\n",
    "\n",
    "# Pandas - Data manipulation and analysis library\n",
    "import pandas as pd\n",
    "\n",
    "# NumPy - mathematical functions on multi-dimensional arrays and matrices\n",
    "import numpy as np\n",
    "\n",
    "# Regular Expression Python module\n",
    "import re\n",
    "\n",
    "# Time - Converting necessary time objects to desried formats\n",
    "import time\n",
    "\n",
    "# Matplotlib - plotting library to create graphs and charts\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Settings for Matplotlib graphs and charts\n",
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = 12, 8\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Authentication OK\n"
     ]
    }
   ],
   "source": [
    "# Set up consumer key and consumer secret\n",
    "consumer_key = \"5Ohy4gwfxvxaV7wPLj4LeRKSx\"\n",
    "consumer_secret = \"lRewDNFpxLOKj4E1g63o5z2gEmticrFC4dM3tHatnh2xKl2iyC\"\n",
    "\n",
    "# Create authentication object and set access token & access token secret\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(\"1326028749842780162-UhEVNo2wusr3W88eK2lfGsZGHV6fAM\", \n",
    "    \"N06snCbxez6OYTRZAQ33SGYybtPOoIVXoyTicWPqZOZjQ\")\n",
    "\n",
    "# Create the API object while passing in the auth information\n",
    "api = tweepy.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True)\n",
    "try:\n",
    "    api.verify_credentials()\n",
    "    print(\"Authentication OK\")\n",
    "except:\n",
    "    print(\"Error during authentication\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build function to get tweets for McPlant, McD, competitors, and vegan market"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Cursor function to get all the tweets mentioning kfc + vegan meat combination\n",
    "\n",
    "def GetTweets(brands, number_of_tweets):\n",
    "    meat_catalogue = [\"vegan\", \"meat free\", \"vegetarian\", \"veg\", \"meatless\", \"plant based\", \"veggie\"]\n",
    "#     meat_catalogue = [\"plant\", \"vegan\", \"impossible\", \"Beyond\",\n",
    "#                       \"gardein\", \"meat-free\", \"vegetarian\", \"vegetarians\", \n",
    "#                       \"veg\", \"meatless\", \"faux-meat\", \"artificial\", \n",
    "#                       \"plant based meat\",  \"meat substitute\",  \"fake meat\", \n",
    "#                       \"faux meat\", \"meat free\", \"plant-based meat\", \"imitation meat\", \"veggie\"]\n",
    "    \n",
    "    query = []\n",
    "    for brand in brands:\n",
    "        query.append(brand)\n",
    "        for combo in meat_catalogue:\n",
    "             query.append(brand+\" \"+combo)\n",
    "\n",
    "                \n",
    "    # Creation of query method using parameters with a for loop\n",
    "    tweets_list = []\n",
    "    for keyword in query:\n",
    "        tweets = tweepy.Cursor(api.search,q=keyword,lang='en',tweet_mode='extended').items(number_of_tweets)\n",
    "        # Pulling information from tweets iterable object\n",
    "        # Add or remove tweet information you want in the below list comprehension\n",
    "        for tweet in tweets:\n",
    "            tweets_list.append([tweet.full_text, tweet.created_at, tweet.id_str,\n",
    "                         tweet.user.screen_name, tweet.user.id_str, tweet.user.location, tweet.retweet_count,\n",
    "                         tweet.favorite_count, tweet.in_reply_to_status_id_str, tweet.in_reply_to_user_id_str,\n",
    "                         tweet.user.followers_count, tweet.user.friends_count, tweet.coordinates, tweet.place,\n",
    "                         tweet.entities['hashtags'], keyword])\n",
    "\n",
    "    # Creation of dataframe from tweets_list\n",
    "    # Add or remove columns as you remove tweet information\n",
    "    tweets_df = pd.DataFrame(tweets_list,columns=['Tweet Text', 'Tweet Datetime', 'Tweet Id', \n",
    "                                                  'Twitter @ Name', 'User Id', 'User Location',\n",
    "                                                  'Retweets', 'Favorites', 'Replied Tweet Id', \n",
    "                                                  'Replied Tweet User Id Str', 'User Follower Counts', \n",
    "                                                  'User Following Counts', 'Tweet Coordinates', \n",
    "                                                  'Place Info','Hashtags','Keyword'])\n",
    "\n",
    "    # Checks if there are coordinates attached to tweets, if so extracts them\n",
    "    tweets_df['Tweet Coordinates'] = tweets_df.apply(extract_coordinates,axis=1)\n",
    "\n",
    "    # Checks if there is place information available, if so extracts them\n",
    "    tweets_df['Place Info'] = tweets_df.apply(extract_place,axis=1)\n",
    "    \n",
    "    return tweets_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build functions to extract coordinates, place info, categorize country and clean emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function created to extract coordinates from tweet if it has coordinate info\n",
    "# Tweets tend to have null so important to run check\n",
    "\n",
    "def extract_coordinates(row):\n",
    "    if row['Tweet Coordinates']:\n",
    "        return row['Tweet Coordinates']['coordinates']\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function created to extract place such as city, state or country from tweet if it has place info\n",
    "# Tweets tend to have null so important to run check\n",
    "\n",
    "def extract_place(row):\n",
    "    if row['Place Info']:\n",
    "        return row['Place Info'].full_name\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert emojis to text\n",
    "import emoji\n",
    "def cleanemoji(tweet):\n",
    "    tweet = emoji.demojize(tweet)\n",
    "    tweet = tweet.replace(\":\", \" \") #to remove all : from :flushed_face:\n",
    "    tweet = tweet.replace(\"_\", \" \") #to remove _ from thumbs_up\n",
    "    tweet = \" \".join(tweet.split())\n",
    "    return tweet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build function to extract hashtags from the uncleaned hashtag formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function created to extract hashtag if it has any hashtags info\n",
    "# Tweets tend to have null so important to run check\n",
    "\n",
    "def get_hash_tag_list(hashtags):\n",
    "    hash_tag_list=[]\n",
    "    if hashtags != []:\n",
    "        for i in hashtags:\n",
    "            hash_tag_list.append(i['text'])\n",
    "        return hash_tag_list\n",
    "    else:\n",
    "        return None\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "************************************************************************************************************************************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get all KFC tweets and relevant information and save it to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all KFC tweets and save it to dataframe object\n",
    "kfc_brands = ['kfc']\n",
    "kfc_tweets_df = GetTweets(kfc_brands, 20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass KFC Tweets for hashtag extraction\n",
    "\n",
    "kfc_tweets_df['Hashtag List'] = kfc_tweets_df['Hashtags'].apply(get_hash_tag_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the file to csv\n",
    "\n",
    "kfc_tweets_df.to_csv('kfc_tweets.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get all Burger King tweets and relevant information and save it to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all Burger King tweets and save it to dataframe object\n",
    "\n",
    "burgerking_brands = ['burger king']\n",
    "burgerking_tweets_df = GetTweets(burgerking_brands, 20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass Burger King Tweets for hashtag extraction\n",
    "\n",
    "burgerking_tweets_df['Hashtag List'] = burgerking_tweets_df['Hashtags'].apply(get_hash_tag_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the file to csv\n",
    "\n",
    "burgerking_tweets_df.to_csv('burgerking_tweets.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get all PizzaHut tweets and relevant information and save it to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all Pizzahut tweets and save it to dataframe object\n",
    "\n",
    "pizzahut_brands = ['pizzahut']\n",
    "pizzahut_tweets_df = GetTweets(pizzahut_brands, 20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass PizzaHut Tweets for hashtag extraction\n",
    "\n",
    "pizzahut_tweets_df['Hashtag List'] = pizzahut_tweets_df['Hashtags'].apply(get_hash_tag_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the file to csv\n",
    "\n",
    "pizzahut_tweets_df.to_csv('pizzahut_tweets.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get all McDonald's tweets and relevant information and save it to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all McDonald's tweets and save it to dataframe object\n",
    "\n",
    "mcd_brands = [\"McDonald's\"]\n",
    "mcd_tweets_df = GetTweets(mcd_brands, 20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass McDonald's Tweets for hashtag extraction\n",
    "\n",
    "mcd_tweets_df['Hashtag List'] = mcd_tweets_df['Hashtags'].apply(get_hash_tag_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the file to csv\n",
    "\n",
    "mcd_tweets_df.to_csv('mcd_tweets.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get all McPlant tweets and relevant information and save it to csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get all Vegan Market tweets and relevant information and save it to csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to create a cleantext function that will:\n",
    "- remove mentions\n",
    "- remove hashtags\n",
    "- remove retweets\n",
    "- remove urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data cleansing without stemming - For Word Cloud Visualization\n",
    "\n",
    "def cleantext_cloud(text):\n",
    "    import re\n",
    "    import contractions\n",
    "    text = text.lower().strip() # lowercase all letters\n",
    "    text = re.sub(r'@[A-za-z0-9]+','',text) # remove @mentions\n",
    "    text = re.sub(r'RT[\\s]+','',text) #remove retweet\n",
    "    text = re.sub(r'https?://\\S+','',text) #remove hyperlink\n",
    "    text = re.sub(r'#','',text) #remove #\n",
    "\n",
    "    text = text.replace('\\n',' ')\n",
    "\n",
    "    text = contractions.fix(text)\n",
    "\n",
    "    text = re.sub(\"[.,!?:;-=*\\\"'“()_]\", \"\", text) # remove puntucation\n",
    "    text = re.sub(\"[0123456789]\", \"\", text) # remove number\n",
    "\n",
    "    text = text.replace('  ','')\n",
    "\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data cleansing with stemming - For Tweets\n",
    "\n",
    "def cleantext(text):\n",
    "\n",
    "    text = text.lower().strip() # lowercase all letters\n",
    "    text = re.sub(r'@[A-za-z0-9]+','',text) # remove @mentions\n",
    "    text = re.sub(r'RT[\\s]+','',text) #remove retweet\n",
    "    text = re.sub(r'https?://\\S+','',text) #remove hyperlink\n",
    "    text = re.sub(r'#','',text) #remove #\n",
    "\n",
    "    text = text.replace('\\n',' ')\n",
    "\n",
    "    text = contractions.fix(text)\n",
    "\n",
    "    stemmer = SnowballStemmer(language='english') # Stemming for prediction\n",
    "    text = stemmer.stem(text)\n",
    "    text = re.sub(\"[.,!?:;-=*\\\"'“()_]\", \"\",text) # remove puntucation\n",
    "    text = re.sub(\"[0123456789]\", \"\", text) # remove number\n",
    "\n",
    "    text = text.replace('  ','')\n",
    "\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### For KFC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Apply cleantext across all Tweets (KFC)\n",
    "\n",
    "kfc_tweets_df['Clean_text'] = kfc_tweets_df['Tweet Text'].apply(cleantext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Demojize the tweets along Clean_text column\n",
    "\n",
    "kfc_tweets_df['Clean_text'] = kfc_tweets_df['Clean_text'].apply(cleanemoji)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Drop NA rows with no Tweet ID\n",
    "\n",
    "kfc_tweets_df = kfc_tweets_df.dropna(thresh=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Create another KFC pure brand tweets with filtering Keyword = kfc only for brand level analysis\n",
    "\n",
    "kfc_brand_df = kfc_df.query(\"Keyword == 'kfc'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### For Burger King"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Apply cleantext across all Tweets (Burger King)\n",
    "\n",
    "burgerking_tweets_df['Clean_text'] = burgerking_tweets_df['Tweet Text'].apply(cleantext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Demojize the tweets along Clean_text column\n",
    "\n",
    "burgerking_tweets_df['Clean_text'] = burgerking_tweets_df['Clean_text'].apply(cleanemoji)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Drop NA rows with no Tweet ID\n",
    "\n",
    "burgerking_tweets_df = burgerking_tweets_df.dropna(thresh=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Create another Burger King pure brand tweets with filtering Keyword = burgerking only for brand level analysis\n",
    "\n",
    "burgerking_brand_df = burgerking_tweets_df.query(\"Keyword == 'burger king'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### For PizzaHut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Apply cleantext across all Tweets (PizzaHut)\n",
    "\n",
    "pizzahut_tweets_df['Clean_text'] = pizzahut_tweets_df['Tweet Text'].apply(cleantext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Demojize the tweets along Clean_text column\n",
    "\n",
    "pizzahut_tweets_df['Clean_text'] = pizzahut_tweets_df['Clean_text'].apply(cleanemoji)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Drop NA rows with no Tweet ID\n",
    "\n",
    "pizzahut_tweets_df = pizzahut_tweets_df.dropna(thresh=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Create another PizzaHut pure brand tweets with filtering Keyword = pizzahut only for brand level analysis\n",
    "\n",
    "pizzahut_brand_df = pizzahut_tweets_df.query(\"Keyword == 'pizzahut'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### For Mcdonald's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Apply cleantext across all Tweets (McDonald's)\n",
    "\n",
    "mcd_tweets_df['Clean_text'] = mcd_tweets_df['Tweet Text'].apply(cleantext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Demojize the tweets along Clean_text column\n",
    "\n",
    "mcd_tweets_df['Clean_text'] = mcd_tweets_df['Clean_text'].apply(cleanemoji)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Drop NA rows with no Tweet ID\n",
    "\n",
    "mcd_tweets_df = mcd_tweets_df.dropna(thresh=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Create another McDonald's pure brand tweets with filtering Keyword = McDonald's only for brand level analysis\n",
    "\n",
    "mcd_brand_df = mcd_df.query('Keyword == \"McDonald\\'s\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### For McPlant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Apply cleantext across all Tweets (McPlant)\n",
    "\n",
    "mcplant_tweets_df['Clean_text'] = mcplant_tweets_df['Tweet Text'].apply(cleantext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Demojize the tweets along Clean_text column\n",
    "\n",
    "mcplant_tweets_df['Clean_text'] = mcplant_tweets_df['Clean_text'].apply(cleanemoji)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Drop NA rows with no Tweet ID\n",
    "\n",
    "mcplant_tweets_df = mcplant_tweets_df.dropna(thresh=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Create another McPlant pure brand tweets with filtering Keyword = McPlant only for McPlant level analysis\n",
    "\n",
    "mcplant_df = mcplant_tweets_df.query('Keyword == \"McPlant\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### For Vegan Market"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Apply cleantext across all Tweets (Vegan Market)\n",
    "\n",
    "vegan_tweets_df['Clean_text'] = vegan_tweets_df['Tweet Text'].apply(cleantext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Demojize the tweets along Clean_text column\n",
    "\n",
    "vegan_tweets_df['Clean_text'] = vegan_tweets_df['Clean_text'].apply(cleanemoji)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Drop NA rows with no Tweet ID\n",
    "\n",
    "vegan_tweets_df = vegan_tweets_df.dropna(thresh=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "************************************************************************************************************************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine all pure brand dataframes for sentiment analysis through LSTM model later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate all brand dfs for getting sentiment scores through the LSTM model\n",
    "\n",
    "combined_brand_df = pd.concat([kfc_brand_df, burgerking_brand_df, pizzahut_brand_df, mcd_brand_df],axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "********************************************************************************************************************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM Model (Transfer Learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load LSTM model for prediction\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "model = tf.keras.models.load_model(\"twitter_sentiment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open tokenizer\n",
    "\n",
    "import pickle\n",
    "\n",
    "with open('tokenizer.pickle', 'rb') as handle:\n",
    "    tokenizer = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****************************************************************************************************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label Classification under each level - McPlant, McDonald's, Brand/Competitors, Vegan Market"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build the sentiment classifcation function first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classify sentiment class function\n",
    "\n",
    "def sentiment_class(score):\n",
    "    if score >= 0.5:\n",
    "        return \"Positive\"\n",
    "    else:\n",
    "        return \"Negative\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. For McPlant Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass cleaned tweets in McPlant dataframe to the LSTM model \n",
    "\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "testArr = mcplant_df.Clean_text.to_numpy()\n",
    "input_tok = pad_sequences(tokenizer.texts_to_sequences(testArr),maxlen = 200)\n",
    "scores = model.predict(input_tok, verbose=1, batch_size=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate sentiment scores for each tweet\n",
    "\n",
    "score_df = pd.DataFrame(scores, columns=['Score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update McPlant dataframe for McPlant analysis\n",
    "\n",
    "mcplant_df = mcplant_df.reset_index(drop=True)\n",
    "score_df = score_df.reset_index(drop=True)\n",
    "mcplant_final_df = pd.concat([mcplant_df, score_df], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. For Brand/Competitor Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass cleaned tweets in combined brand dataframe to the LSTM model \n",
    "\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "testArr = combined_brand_df.Clean_text.to_numpy()\n",
    "input_tok = pad_sequences(tokenizer.texts_to_sequences(testArr),maxlen = 200)\n",
    "scores = model.predict(input_tok, verbose=1, batch_size=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate sentiment scores for each tweet\n",
    "\n",
    "score_df = pd.DataFrame(scores, columns=['Score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create final dataframe for overall brand analysis\n",
    "\n",
    "mcplant_df = mcplant_df.reset_index(drop=True)\n",
    "score_df = score_df.reset_index(drop=True)\n",
    "brands_final_df = pd.concat([combined_brand_df, score_df], axis=1)\n",
    "\n",
    "brands_final_df.to_csv('brands_final_df.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
